{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15bcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI                               #Importing wrapper library for reddit(Pushshift)\n",
    "import datetime as dt                                       #Importing library for date management\n",
    "import pandas as pd                                         #Importing library for data manipulation in python\n",
    "import matplotlib.pyplot as plt                             #Importing library for creating interactive visualizations in Python\n",
    "from pprint import pprint         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99b9e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)                  #Configuration for pandas to show all columns on dataframe\n",
    "api = PushshiftAPI()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ccf45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with subreddit\n",
    "\n",
    "# def data_prep_posts(subreddit, start_time, end_time, filters, keyword, limit):\n",
    "#     if(len(filters) == 0):\n",
    "#         filters = ['id', 'author', 'created_utc',\n",
    "#                    'domain', 'url',\n",
    "#                    'title', 'num_comments']                 #We set by default some columns that will be useful for data analysis\n",
    "\n",
    "#     posts = list(api.search_submissions(\n",
    "#         subreddit=subreddit,                                           #We set the subreddit we want to audit\n",
    "#         after=start_time,                                   #Start date\n",
    "#         before=end_time,                                    #End date\n",
    "#         filter=filters,\n",
    "#         q = keyword,\n",
    "#         limit=limit))                                       #Max number of posts we wanto to recieve\n",
    "\n",
    "#     return pd.DataFrame(posts)                              #Return dataframe for analysis\n",
    "\n",
    "#no subreddit\n",
    "\n",
    "def data_prep_posts(start_time, end_time, filters, keyword, limit):\n",
    "    if(len(filters) == 0):\n",
    "        filters = ['id', 'author', 'created_utc',\n",
    "                   'domain', 'url',\n",
    "                   'title', 'num_comments']                 #We set by default some columns that will be useful for data analysis\n",
    "\n",
    "    posts = list(api.search_submissions(                                          #We set the subreddit we want to audit\n",
    "        after=start_time,                                   #Start date\n",
    "        before=end_time,                                    #End date\n",
    "        filter=filters,\n",
    "        q = keyword,\n",
    "        limit=limit))                                       #Max number of posts we wanto to recieve\n",
    "\n",
    "    return pd.DataFrame(posts)                              #Return dataframe for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b0c7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FOR COMMENTS\"\"\"\n",
    "def data_prep_comments(term, start_time, end_time, filters, limit):\n",
    "    if (len(filters) == 0):\n",
    "        filters = ['id', 'author', 'created_utc',\n",
    "                   'body', 'permalink', 'subreddit']\n",
    "                   #We set by default some usefull columns \n",
    "\n",
    "    comments = list(api.search_comments(\n",
    "        q=term,                 #Subreddit we want to audit\n",
    "        after=start_time,       #Start date\n",
    "        before=end_time,        #End date\n",
    "        filter=filters,         #Column names we want to retrieve\n",
    "        limit=limit))           #Max number of comments\n",
    "    return pd.DataFrame(comments) #Return dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474df554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_posts_per_date(df_p, title, xlabel, ylabel):\n",
    "    df_p.groupby([df_p.datetime.dt.date]).count().plot(y='id', rot=45, kind='bar', label='Posts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52720b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEND - FUNCTIONS\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "BEGIN - FUNCTIONS\n",
    "\"\"\"\n",
    "###Function to plot the number of posts per day on the specified subreddit\n",
    "def count_posts_per_date(df_p, title, xlabel, ylabel):\n",
    "    df_p.groupby([df_p.datetime.dt.date]).count().plot(y='id', rot=45, kind='bar', label='Posts')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "###Function to plot the mean of comments per day on the specified subreddit\n",
    "def mean_comments_per_date(df_p, title, xlabel, ylabel):\n",
    "    df_p.groupby([df_p.datetime.dt.date]).mean().plot(y='num_comments', rot=45, kind='line', label='Comments')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "###Function to plot the most active users on the subreddit\n",
    "def most_active_author(df_p, title, xlabel, ylabel, limit):\n",
    "    df_p.groupby([df_p.author]).count()['id'].nlargest(limit).sort_values(ascending=True).plot(y='id', rot=45, kind='barh', label='Users')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "###Function to que the orgin form the crossposting\n",
    "def get_posts_orign(df_p, title, xlabel, ylabel, limit, subreddit):\n",
    "    domains = df_p[(df_p.domain != 'reddit.com') & (df_p.domain != f'self.{subreddit}') & (df_p.domain != 'i.redd.it')]\n",
    "    domains.groupby(by='domain').count()['id'].nlargest(limit).sort_values(ascending=True).plot(kind='barh', rot=45, x='domain', label='# of posts', legend=True, figsize=(8,13))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "###Gets most active subrredits according to search term\n",
    "def get_subreddits(df_p, title, xlabel, ylabel, limit):\n",
    "    df_p.groupby(by='subreddit').count()['id'].nlargest(limit).sort_values(ascending=True).plot(kind='barh', x='subreddit', label='Subreddit', legend=True, figsize=(8,13))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "END - FUNCTIONS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92307fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with no subreddit\n",
    "\n",
    "def main():                   #Name of the subreddit we are auditing\n",
    "    start_time = int(dt.datetime(2022, 11, 1).timestamp())  #We define the starting date for our search\n",
    "    end_time = int(dt.datetime(2022,12, 1).timestamp())   #We define the ending date for our search\n",
    "    filters = [] \n",
    "    keyword = \"AAPL\"\n",
    "    limit = 3000                                         #Number of elelemts we want to recieve\n",
    "\n",
    "    \"\"\"Here we are going to get subreddits for a brief analysis\"\"\"\n",
    "    df_p = data_prep_posts(start_time,\n",
    "                         end_time,filters,keyword,limit)           #Call function for dataframe creation of comments\n",
    "\n",
    "    df_p['datetime'] = df_p['created_utc'].map(\n",
    "        lambda t: dt.datetime.fromtimestamp(t))\n",
    "    df_p = df_p.drop('created_utc', axis=1)                #Drop the column on timestamp format\n",
    "    df_p = df_p.sort_values(by='datetime')                 #Sort the Row by datetime\n",
    "    df_p[\"datetime\"] = pd.to_datetime(df_p[\"datetime\"])    #Convert timestamp format to datetime for data analysis\n",
    "\n",
    "\n",
    "    df_p.to_csv(f'nov-{keyword}_dataset_all_posts.csv', sep=',', # Save the dataset on a csv file for future analysis\n",
    "                header=True, index=False, columns=[\n",
    "            'id', 'author', 'datetime', 'domain',\n",
    "            'url', 'title', 'num_comments'\n",
    "        ])\n",
    "\n",
    "#     count_posts_per_date(df_p, 'Post per day', 'Days',     #Function to plot the number of posts per day on the specified subreddit\n",
    "#                          'posts')\n",
    "#     mean_comments_per_date(df_p,                           #Function to plot the mean of comments per day on the specified subreddit\n",
    "#                            'Average comments per day',\n",
    "#                            'Days', 'comments')\n",
    "#     most_active_author(df_p, 'Most active users',          #Function to plot the most active users on the subreddit\n",
    "#                        'Posts', 'Users', 10)\n",
    "#     get_posts_orign(df_p, 'Origin of crosspostings',       #Function to que the orgin form the crossposting\n",
    "#                     'Crossposts', 'Origins', 10,\n",
    "#                     subreddit)\n",
    "\n",
    "if __name__== \"__main__\" : main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45057c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     subreddit = \"wallstreetbets\"                           #Name of the subreddit we are auditing\n",
    "#     start_time = int(dt.datetime(2022, 11, 1).timestamp())  #We define the starting date for our search\n",
    "#     end_time = int(dt.datetime(2022,12, 1).timestamp())   #We define the ending date for our search\n",
    "#     filters = [] \n",
    "#     keyword = \"AAPL\"\n",
    "#     limit = 3000                                         #Number of elelemts we want to recieve\n",
    "\n",
    "#     \"\"\"Here we are going to get subreddits for a brief analysis\"\"\"\n",
    "#     df_p = data_prep_posts(subreddit, start_time,\n",
    "#                          end_time,filters,keyword,limit)           #Call function for dataframe creation of comments\n",
    "\n",
    "#     df_p['datetime'] = df_p['created_utc'].map(\n",
    "#         lambda t: dt.datetime.fromtimestamp(t))\n",
    "#     df_p = df_p.drop('created_utc', axis=1)                #Drop the column on timestamp format\n",
    "#     df_p = df_p.sort_values(by='datetime')                 #Sort the Row by datetime\n",
    "#     df_p[\"datetime\"] = pd.to_datetime(df_p[\"datetime\"])    #Convert timestamp format to datetime for data analysis\n",
    "\n",
    "\n",
    "#     df_p.to_csv(f'nov-{keyword}_dataset_{subreddit}_posts.csv', sep=',', # Save the dataset on a csv file for future analysis\n",
    "#                 header=True, index=False, columns=[\n",
    "#             'id', 'author', 'datetime', 'domain',\n",
    "#             'url', 'title', 'num_comments'\n",
    "#         ])\n",
    "\n",
    "#     count_posts_per_date(df_p, 'Post per day', 'Days',     #Function to plot the number of posts per day on the specified subreddit\n",
    "#                          'posts')\n",
    "#     mean_comments_per_date(df_p,                           #Function to plot the mean of comments per day on the specified subreddit\n",
    "#                            'Average comments per day',\n",
    "#                            'Days', 'comments')\n",
    "#     most_active_author(df_p, 'Most active users',          #Function to plot the most active users on the subreddit\n",
    "#                        'Posts', 'Users', 10)\n",
    "#     get_posts_orign(df_p, 'Origin of crosspostings',       #Function to que the orgin form the crossposting\n",
    "#                     'Crossposts', 'Origins', 10,\n",
    "#                     subreddit)\n",
    "\n",
    "# if __name__== \"__main__\" : main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c96438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4318c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d42524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad2856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d430c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a68f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0d390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902f260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e154d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
