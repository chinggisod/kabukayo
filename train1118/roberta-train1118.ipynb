{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7773c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk import inference\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def preprocess(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub('[^A-z]', ' ', sentence)\n",
    "    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                        'even though', 'yet']\n",
    "    stop_words = [z for z in stop_words if z not in negative]\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n",
    "    return ' '.join([x for x in preprocessed_tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "916c4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.read_csv('training_aapl.csv', low_memory=False)\n",
    "df['label'] = df['label'].astype(int)\n",
    "df = df.drop(['date'], axis=1)\n",
    "df['title'] = df['title'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5b9800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.iloc[:450]\n",
    "val_data = df.iloc[450:500]\n",
    "test_data = df.iloc[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bf8334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Train Data': train_data, 'Validation Data': val_data, 'Test Data': test_data} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41973449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_x, train_y = ros.fit_resample(np.array(train_data['title']).reshape(-1, 1), np.array(train_data['label']).reshape(-1, 1))\n",
    "train = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['title', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "806982f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.OneHotEncoder()\n",
    "y_train= le.fit_transform(np.array(train['label']).reshape(-1, 1)).toarray()\n",
    "y_test= le.fit_transform(np.array(test_data['label']).reshape(-1, 1)).toarray()\n",
    "y_val= le.fit_transform(np.array(val_data['label']).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0c3848a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6a0c4620a64005bdfdfa530925a872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb2079e95ec4e148c91807a197fdcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9406065c2a842cfa261d54f346aa22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc544b4a7ec403e96426d6cebe51e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "242285da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.reset_index(inplace = True, drop = True)\n",
    "val_data.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6bb16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encode(data,maximum_length) :\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  \n",
    "\n",
    "  for i in range(len(data.title)):\n",
    "      encoded = tokenizer.encode_plus(\n",
    "        \n",
    "        data.title[i],\n",
    "        add_special_tokens=True,\n",
    "        max_length=maximum_length,\n",
    "        pad_to_max_length=True,\n",
    "        \n",
    "        return_attention_mask=True,\n",
    "        \n",
    "      )\n",
    "      \n",
    "      input_ids.append(encoded['input_ids'])\n",
    "      attention_masks.append(encoded['attention_mask'])\n",
    "  return np.array(input_ids),np.array(attention_masks)\n",
    "\n",
    "max_len = max([len(x.split()) for x in train_data['title']])\n",
    "train_input_ids,train_attention_masks = roberta_encode(train, max_len)\n",
    "test_input_ids,test_attention_masks = roberta_encode(test_data, max_len)\n",
    "val_input_ids,val_attention_masks = roberta_encode(val_data, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5194a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, max_len):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "    output = bert_model([input_ids,attention_masks])\n",
    "    output = output[1]\n",
    "\n",
    "    output = tf.keras.layers.Dense(6, activation='softmax')(output)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4c355af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525e7ce24bf644e4a0f4ee9d13ca67ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/657M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 11:19:51.423276: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-18 11:19:51.424701: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFRobertaModel\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "model = create_model(roberta_model, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a85152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(result):\n",
    "    sns.barplot(x = 'Category', y = 'Confidence', data = result)\n",
    "    plt.xlabel('Categories', size=14)\n",
    "    plt.ylabel('Confidence', size=14)\n",
    "    plt.title('Emotion Classification', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425af3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
